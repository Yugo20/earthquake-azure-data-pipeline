{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "48779789-6b8e-4c0a-9585-2f5a74240a39",
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {}
    }
   },
   "outputs": [],
   "source": [
    "# Define ADLS Gen2 Storage Account Name\n",
    "# IMPORTANT: Replace 'earthquakedataluke' with your actual ADLS Gen2 storage account name for interactive testing.\n",
    "storage_account_name = \"earthquakedataluke\" # Replace with your storage account name\n",
    "\n",
    "# Uncomment the following lines for Azure Data Factory (ADF) execution:\n",
    "# dbutils.widgets.text(\"storage_account_name\", \"\", \"Storage Account Name\")\n",
    "# storage_account_name = dbutils.widgets.get(\"storage_account_name\")\n",
    "\n",
    "print(f\"Using storage_account_name: {storage_account_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "ac06ba16-db9d-4864-9d82-ea20f734d452",
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "# Remove this before running Data Factory Pipeline\n",
    "start_date = date.today() - timedelta(1)\n",
    "\n",
    "bronze_adls = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net/\"\n",
    "silver_adls = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "f7961e51-6a4b-48aa-98fb-2e07014fe10c",
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' Data Factory\n",
    "dbutils.widgets.text(\"storage_account_name\", \"\", \"Storage Account Name\")\n",
    "storage_account_name_direct = dbutils.widgets.get(\"storage_account_name\") # If passed directly by ADF\n",
    "import json\n",
    "\n",
    "# Retrieve the bronze_params directly as a widget\n",
    "bronze_params = dbutils.widgets.get(\"bronze_params\")\n",
    "print(f\"Raw bronze_params: {bronze_params}\")\n",
    "\n",
    "# Parse the JSON string\n",
    "output_data = json.loads(bronze_params)\n",
    "storage_account_name_param = output_data.get(\"storage_account_name\", \"\") # From Bronze params\n",
    "\n",
    "# Determine storage_account_name to use (prefer direct ADF param if available)\n",
    "if 'storage_account_name_direct' in locals() and storage_account_name_direct:\n",
    "    storage_account_name = storage_account_name_direct\n",
    "elif 'storage_account_name_param' in locals() and storage_account_name_param:\n",
    "    storage_account_name = storage_account_name_param\n",
    "else:\n",
    "    # Fallback or error if neither is available and it's required\n",
    "    print(\"Warning: storage_account_name not found in ADF params or bronze_params. Using pre-defined or default.\")\n",
    "    # Ensure storage_account_name is defined, e.g. from an earlier cell for interactive, or raise error\n",
    "    if 'storage_account_name' not in locals():\n",
    "       storage_account_name = \"YOUR_DEFAULT_STORAGE_ACCOUNT_NAME_HERE\" # Should be set by widget cell in interactive\n",
    "print(f\"Storage account name for ADF context: {storage_account_name}\")\n",
    "\n",
    "# Access individual variables\n",
    "start_date = output_data.get(\"start_date\", \"\")\n",
    "end_date = output_data.get(\"end_date\", \"\")\n",
    "# Update ADLS paths if they are also sourced from bronze_params, using the determined storage_account_name\n",
    "bronze_adls = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net/\"\n",
    "silver_adls = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/\"\n",
    "gold_adls = f\"abfss://gold@{storage_account_name}.dfs.core.windows.net/\" # Assuming gold_adls might also be needed\n",
    "\n",
    "print(f\"Start Date: {start_date}, Bronze ADLS: {bronze_adls}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "78f3ec41-67d6-43e4-be7b-fff8c913b983"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnull, when\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "75eaa9de-a294-474e-b485-6862f0a2aff2"
    }
   },
   "outputs": [],
   "source": [
    "# Load the JSON data into a Spark DataFrame\n",
    "df = spark.read.option(\"multiline\", \"true\").json(f\"{bronze_adls}{start_date}_earthquake_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "8535c26f-0c07-4985-91e6-138731125d93",
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {}
    }
   },
   "source": [
    "### Data Validation and Null Handling\n",
    "- Null values for `longitude`, `latitude`, and `time` are preserved as `null`. Downstream processes should be prepared to handle these nulls appropriately.\n",
    "- The original source (USGS API) may provide nulls or omit fields, which Spark will interpret as null when reading the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "9dfbf91f-6a36-42c6-b3a5-4f6222b9cdb0"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "08a12191-5d65-4409-a17b-dc13721db2a3"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "2f477bf7-aa46-4d2d-bbbd-83dff93028bd"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape earthquake data\n",
    "df = (\n",
    "    df\n",
    "    .select(\n",
    "        'id',\n",
    "        col('geometry.coordinates').getItem(0).alias('longitude'),\n",
    "        col('geometry.coordinates').getItem(1).alias('latitude'),\n",
    "        col('geometry.coordinates').getItem(2).alias('elevation'),\n",
    "        col('properties.title').alias('title'),\n",
    "        col('properties.place').alias('place_description'),\n",
    "        col('properties.sig').alias('sig'),\n",
    "        col('properties.mag').alias('mag'),\n",
    "        col('properties.magType').alias('magType'),\n",
    "        col('properties.time').alias('time'),\n",
    "        col('properties.updated').alias('updated')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "fb1bc1b6-d0fd-4e3c-bfa0-87e38c3cd27b"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "a6bf076c-dfa6-4dc5-82bd-7bb9beeab778"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "23e4bf83-507d-444b-b315-49287e79dc71",
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'time' and 'updated' to timestamp from Unix time\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('time', (col('time') / 1000).cast(TimestampType()))\n",
    "    .withColumn('updated', (col('updated') / 1000).cast(TimestampType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "fc2f0d46-95b0-42d5-8655-4dc90860b00c",
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "21b7e5d1-12de-4afa-b4f1-13b5c66dc75d",
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the transformed DataFrame to the Silver container\n",
    "silver_output_path = f\"{silver_adls}earthquake_events_silver/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "c6d79f24-e7c7-4a8a-a936-9d4dc9e151a3",
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Append DataFrame to Silver container in Parquet format\n",
    "df.write.mode('append').parquet(silver_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "23e0c26a-163d-4519-b11d-e86c2d15446d",
     "cellMetadata": {},
     "inputWidgets": {},
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' Data Factory\n",
    "# Also pass storage_account_name to Gold notebook if needed, or ensure ADF passes it to Gold directly\n",
    "output_to_gold = {\n",
    "    \"silver_output_path\": silver_output_path,\n",
    "    \"storage_account_name\": storage_account_name, # Pass it along\n",
    "    \"start_date\": start_date # Pass start_date for partitioning or logging if Gold needs it\n",
    "}\n",
    "dbutils.notebook.exit(json.dumps(output_to_gold))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
