{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "ef92d5a9-9b34-4a98-857c-52439c926923"
    }
   },
   "outputs": [],
   "source": [
    "# Define ADLS Gen2 Storage Account Name\n",
    "# IMPORTANT: Replace 'earthquakedataluke' with your actual ADLS Gen2 storage account name for interactive testing.\n",
    "storage_account_name = \"earthquakedataluke\" # Replace with your storage account name\n",
    "\n",
    "# Uncomment the following lines for Azure Data Factory (ADF) execution:\n",
    "# dbutils.widgets.text(\"storage_account_name\", \"\", \"Storage Account Name\")\n",
    "# storage_account_name = dbutils.widgets.get(\"storage_account_name\")\n",
    "\n",
    "print(f\"Using storage_account_name: {storage_account_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "6983d27b-7bda-45a1-b081-9d5550f7f80e"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "# Remove this before running Data Factory Pipeline\n",
    "start_date = date.today() - timedelta(1)\n",
    "\n",
    "silver_adls = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/\"\n",
    "gold_adls = f\"abfss://gold@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "silver_data = f\"{silver_adls}earthquake_events_silver/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "e91fb293-6bb6-467e-8c4f-ce2d7bc7ec39"
    }
   },
   "outputs": [],
   "source": [
    "''' Data Factory\n",
    "\n",
    "import json\n",
    "\n",
    "# Get base parameters\n",
    "dbutils.widgets.text(\"bronze_params\", \"\")\n",
    "dbutils.widgets.text(\"silver_params\", \"\")\n",
    "dbutils.widgets.text(\"storage_account_name\", \"\", \"Storage Account Name\")\n",
    "\n",
    "bronze_params = dbutils.widgets.get(\"bronze_params\")\n",
    "silver_params = dbutils.widgets.get(\"silver_params\")\n",
    "storage_account_name_direct = dbutils.widgets.get(\"storage_account_name\") # If passed directly by ADF\n",
    "silver_data_params = {}\n",
    "if silver_params and isinstance(silver_params, str) and silver_params.strip().startswith('{'):\n",
    "    try:\n",
    "        silver_data_params = json.loads(silver_params)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Could not decode silver_params as JSON: {silver_params}\")\n",
    "        # silver_params might be just a path as in original notebook, handle accordingly\n",
    "        # If it's just a path, storage_account_name should come from bronze_data or direct widget\n",
    "        pass\n",
    "storage_account_name_silver = silver_data_params.get(\"storage_account_name\", \"\") # From silver_params (if JSON and contains the key)\n",
    "\n",
    "# Debug: Print the raw input values for troubleshooting\n",
    "print(f\"Raw bronze_params: {bronze_params}\")\n",
    "print(f\"Raw silver_params: {silver_params}\")\n",
    "\n",
    "# Parse the JSON string\n",
    "bronze_data = json.loads(bronze_params)\n",
    "\n",
    "# Access individual variables\n",
    "storage_account_name_bronze = bronze_data.get(\"storage_account_name\", \"\") # From bronze_data\n",
    "start_date = bronze_data.get(\"start_date\", \"\")\n",
    "end_date = bronze_data.get(\"end_date\", \"\")\n",
    "silver_adls_param = bronze_data.get(\"silver_adls\", \"\") # Path from bronze_data\n",
    "gold_adls_param = bronze_data.get(\"gold_adls\", \"\")   # Path from bronze_data\n",
    "silver_data_input_path = silver_params # This is the direct input from silver_params widget\n",
    "\n",
    "# Determine storage_account_name to use\n",
    "if 'storage_account_name_direct' in locals() and storage_account_name_direct:\n",
    "    storage_account_name = storage_account_name_direct\n",
    "    print(f\"Using direct ADF storage_account_name: {storage_account_name}\")\n",
    "elif 'storage_account_name_silver' in locals() and storage_account_name_silver:\n",
    "    storage_account_name = storage_account_name_silver\n",
    "    print(f\"Using storage_account_name from silver_params: {storage_account_name}\")\n",
    "elif 'storage_account_name_bronze' in locals() and storage_account_name_bronze:\n",
    "    storage_account_name = storage_account_name_bronze\n",
    "    print(f\"Using storage_account_name from bronze_data: {storage_account_name}\")\n",
    "else:\n",
    "    print(\"Warning: storage_account_name not found in ADF or upstream params. Using pre-defined or default from interactive cell.\")\n",
    "    if 'storage_account_name' not in locals(): # Fallback to value from widget cell if running interactively\n",
    "        storage_account_name = \"earthquakedataluke\" # Default if not found anywhere else\n",
    "print(f\"Final storage_account_name for Gold: {storage_account_name}\")\n",
    "\n",
    "# Re-construct ADLS paths using the determined storage_account_name\n",
    "silver_adls = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/\"\n",
    "gold_adls = f\"abfss://gold@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "# Determine the correct silver_data path to read from\n",
    "if isinstance(silver_data_params, dict) and silver_data_params.get('silver_output_path'):\n",
    "    silver_data = silver_data_params['silver_output_path']\n",
    "    print(f\"Using silver_output_path from parsed silver_params (JSON): {silver_data}\")\n",
    "elif silver_data_input_path and silver_data_input_path.startswith('abfss://'):\n",
    "    silver_data = silver_data_input_path\n",
    "    print(f\"Using silver_params widget value as full silver_data path: {silver_data}\")\n",
    "else:\n",
    "    # Fallback if silver_data_input_path is not a full path (e.g. only a date or relative part)\n",
    "    silver_data = f\"{silver_adls}earthquake_events_silver/\"\n",
    "    print(f\"Defaulting silver_data path using determined silver_adls: {silver_data}\")\n",
    "\n",
    "# Debug: Print the extracted values for verification\n",
    "print(f\"Start Date: {start_date}, End Date: {end_date}\")\n",
    "print(f\"Silver ADLS (used for constructing silver_data path if needed): {silver_adls}\")\n",
    "print(f\"Gold ADLS Path: {gold_adls}\")\n",
    "print(f\"Final silver_data path to be read: {silver_data}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "e44e2d7b-08e7-41b0-a110-5f9cc0643077"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "# Ensure the below library is installed on your cluster\n",
    "import reverse_geocoder as rg\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "45dca249-dfb5-4dc8-bd87-f9f0a0eea794"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(silver_data).filter(col('time') > start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "7bc3fae0-974b-455e-a9c9-8c0ee218fc6e"
    }
   },
   "outputs": [],
   "source": [
    "# The problem is caused by the Python UDF (reverse_geocoder) being a bottleneck due to its non-parallel nature and high computational cost per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "3c807d0e-1f0f-4113-be4f-414406a7bd4d"
    }
   },
   "outputs": [],
   "source": [
    "def get_country_code(lat, lon):\n",
    "    \"\"\"\n",
    "    Retrieve the country code for a given latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "    lat (float or str): Latitude of the location.\n",
    "    lon (float or str): Longitude of the location.\n",
    "\n",
    "    Returns:\n",
    "    str: Country code of the location, retrieved using the reverse geocoding API.\n",
    "\n",
    "    Example:\n",
    "    >>> get_country_details(48.8588443, 2.2943506)\n",
    "    'FR'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        coordinates = (float(lat), float(lon))\n",
    "        result = rg.search(coordinates)[0].get('cc')\n",
    "        print(f\"Processed coordinates: {coordinates} -> {result}\")\n",
    "        # Note: print() statements in UDFs are for interactive debugging; output may not appear on driver or be reliably collected from worker logs in production.\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {lat}, {lon} -> {str(e)}\")\n",
    "        # Note: print() statements in UDFs are for interactive debugging; output may not appear on driver or be reliably collected from worker logs in production.\n",
    "        return \"ERROR_GEOCODING\" # Changed from None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "f50a8441-ae75-4ce7-beb5-6329bf365055"
    }
   },
   "outputs": [],
   "source": [
    "# registering the udfs so they can be used on spark dataframes\n",
    "get_country_code_udf = udf(get_country_code, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "7a1cfc01-2afa-4c93-839c-55f9553c5b77"
    }
   },
   "outputs": [],
   "source": [
    "get_country_code(48.8588443, 2.2943506)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "e7f08b31-2540-4bc2-8482-8fbb0e44616f"
    }
   },
   "source": [
    "### IMPORTANT: Performance Consideration for `get_country_code_udf`\n",
    "\n",
    "The `get_country_code_udf` uses the `reverse_geocoder` library within a standard Python UDF. As noted in the project's `guide.md` (Step 7) and in earlier comments, this approach can be a **significant performance bottleneck** for large datasets due to the row-by-row processing nature of standard UDFs and the computational cost of geocoding.\n",
    "\n",
    "**For production or large-scale processing, consider the following optimizations as recommended in `guide.md`:**\n",
    "- Using Pandas UDFs (vectorized UDFs) for better performance with Python native libraries.\n",
    "- Pre-calculating a lookup table for common coordinates if the geographic range is somewhat limited.\n",
    "- Performing geocoding in batches outside of the main Spark transformation if feasible.\n",
    "\n",
    "The `df.limit(100)` line, previously used for faster testing, has been removed to allow full data processing. Ensure your cluster is appropriately sized if running this notebook on large datasets with the current UDF implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "e593bd05-422f-44bb-9848-35a5c317eb81"
    }
   },
   "outputs": [],
   "source": [
    "# adding country_code and city attributes\n",
    "df_with_location = \\\n",
    "                df.\\\n",
    "                    withColumn(\"country_code\", get_country_code_udf(col(\"latitude\"), col(\"longitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "f58201f5-2b42-4fab-add5-f6f1d733c6c5"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "c933dd90-30e5-4c69-b61c-ae3ada39f8b0"
    }
   },
   "outputs": [],
   "source": [
    "df_with_location.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "a2034f97-d2cd-446a-abfd-6824a549e8b5"
    }
   },
   "outputs": [],
   "source": [
    "# adding significance classification\n",
    "df_with_location_sig_class = \\\n",
    "                            df_with_location.\\\n",
    "                                withColumn('sig_class', \n",
    "                                            when(col(\"sig\") < 100, \"Low\").\\\n",
    "                                            when((col(\"sig\") >= 100) & (col(\"sig\") < 500), \"Moderate\").\\\n",
    "                                            otherwise(\"High\")\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "1dc0d3e9-e17f-4194-8821-2b3936907c7a"
    }
   },
   "outputs": [],
   "source": [
    "df_with_location_sig_class.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "04a6b456-4afb-41de-b58e-ca6a6bb93d71"
    }
   },
   "outputs": [],
   "source": [
    "# Save the transformed DataFrame to the Gold container using Parquet format\n",
    "gold_output_path = f\"{gold_adls}earthquake_events_gold/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "nuid": "cbf34037-e51e-4686-bb97-d535c4f29893"
    }
   },
   "outputs": [],
   "source": [
    "# Write DataFrame to Gold container in Parquet format, appending to existing data\n",
    "df_with_location_sig_class.write.mode('append').parquet(gold_output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold Notebook",
   "widgets": {
    "bronze_params": {
     "currentValue": "",
     "nuid": "c6e65ba2-4002-4950-8d1a-ae2620c79322",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "bronze_params",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "bronze_params",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "silver_params": {
     "currentValue": "",
     "nuid": "38805122-0ed7-47d4-90c9-8e3052d9657b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "silver_params",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "silver_params",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
